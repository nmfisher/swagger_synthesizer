{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "sys.path.append(\"/home/hydroxide/projects/bert\")\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "class UtteranceExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\n",
    "\n",
    "     For examples without an answer, the start and end position are -1.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               id,\n",
    "               utterance_tokens,\n",
    "               query_tokens,\n",
    "               answer_start_position=None,\n",
    "               answer_end_position=None,\n",
    "               is_impossible=False):\n",
    "    self.id = id\n",
    "    self.query_tokens = query_tokens\n",
    "    self.utterance_tokens = utterance_tokens\n",
    "    self.answer_start_position = answer_start_position\n",
    "    self.answer_end_position = answer_end_position\n",
    "    self.is_impossible = is_impossible\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.__repr__()\n",
    "\n",
    "  def __repr__(self):\n",
    "    s = \"\"\n",
    "    s += \"qas_id: %s\" % (tokenization.printable_text(self.qas_id))\n",
    "    s += \", utterance_tokens: [%s]\" % (\" \".join(self.utterance_tokens))\n",
    "    s += \", query_tokens: [%s]\" % (\" \".join(self.query_tokens))\n",
    "    if self.answer_start_position:\n",
    "      s += \", answer_start_position: %d\" % (self.answer_start_position)\n",
    "    if self.answer_start_position:\n",
    "      s += \", end_position: %d\" % (self.answer_end_position)\n",
    "    if self.is_impossible:\n",
    "      s += \", is_impossible: %r\" % (self.is_impossible)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLES 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['hi', 'guys'], [0, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv \n",
    "\n",
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def build_char_to_word(text):\n",
    "    prev_is_whitespace = True\n",
    "    char_to_word_offset = []\n",
    "    tokens = []\n",
    "    for c in text:\n",
    "        if is_whitespace(c):\n",
    "            prev_is_whitespace = True\n",
    "        else:\n",
    "            if prev_is_whitespace:\n",
    "                tokens.append(c)\n",
    "            else:\n",
    "                tokens[-1] += c\n",
    "            prev_is_whitespace = False\n",
    "        char_to_word_offset.append(len(tokens) - 1)\n",
    "    return tokens, char_to_word_offset\n",
    "\n",
    "def read_csv_examples(input_file, is_training):\n",
    "    \"\"\"Read an csv file containing an utterance per line into a list of UtteranceExample.\"\"\"\n",
    "    with tf.gfile.Open(input_file, \"r\") as reader:\n",
    "        examples = []\n",
    "        \n",
    "        csvreader = csv.reader(reader, delimiter=',')\n",
    "        for entry in csvreader:\n",
    "            utterance_tokens, utterance_char_to_word_offset = build_char_to_word(entry[0].replace('\"',\"\"))\n",
    "            query_tokens, query_char_to_word_offset = build_char_to_word(entry[1].replace('\"',\"\"))\n",
    "            answer_text = entry[2].replace('\"',\"\")\n",
    "            answer_start = int(entry[3].replace('\"',\"\"))\n",
    "            answer_end = int(entry[4].replace('\"',\"\"))\n",
    "\n",
    "            def can_find(text, offset, length, tokens, char_to_word_offset):\n",
    "                start_position = char_to_word_offset[offset]\n",
    "                end_position = char_to_word_offset[offset + length - 1]\n",
    "                # Only add answers where the text can be exactly recovered from the\n",
    "                # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "                # stuff so we will just skip the example.\n",
    "                #\n",
    "                # Note that this means for training mode, every example is NOT\n",
    "                # guaranteed to be preserved.\n",
    "                actual_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
    "                cleaned_answer_text = \" \".join(tokenization.whitespace_tokenize(text))\n",
    "                if actual_text.find(cleaned_answer_text) == -1:\n",
    "                    tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",actual_text, cleaned_answer_text)\n",
    "                    return None,None\n",
    "                return start_position,end_position\n",
    "\n",
    "            if not (answer_start == -1 and answer_end == -1):\n",
    "                answer_start_position, answer_end_position = can_find(answer_text, answer_start, answer_end - answer_start, utterance_tokens, utterance_char_to_word_offset)\n",
    "                if answer_start_position is None or answer_end_position is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    answer_start_position = -1\n",
    "                    answer_end_position = -1\n",
    "                    orig_answer_text = \"\"\n",
    "                    if answer_start_position is None:\n",
    "                        print(paragraph)\n",
    "                        raise Exception()\n",
    "            example = UtteranceExample(\n",
    "                id=id,\n",
    "                utterance_tokens=utterance_tokens,\n",
    "                query_tokens=query_tokens,\n",
    "                answer_start_position=answer_start_position,\n",
    "                answer_end_position=answer_end_position,\n",
    "                is_impossible=answer_start_position==-1)\n",
    "            examples.append(example)\n",
    "            print(\"EXAMPLES {0}\".format(len(examples)))\n",
    "            return examples\n",
    "        \n",
    "read_csv_examples(\"/home/hydroxide/projects/swagger-data-synthesizer/commands.csv\", True)\n",
    "build_char_to_word(\"hi guys\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
